# Homework 3-2

###
-----
- The `stream-processing.py` uses `pyspark` to consume streaming data from a Kafka topic called `stock-analyzer`, calculate the average price for a specific stock for every 5 seconds and send the results back to a new Kafka topic called `average-stock-price`.

- The program can be started by executing
  ```
  spark-submit --jars spark-streaming-kafka-0-8-assembly_2.11-2.0.1.jar stream-processing.py 192.168.99.100:9092 stock-analyzer average-stock-price
  ```
  shows
  ```
  Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/11/07 21:45:30 INFO SparkContext: Running Spark version 2.0.1
16/11/07 21:45:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/11/07 21:45:30 INFO SecurityManager: Changing view acls to: yang
16/11/07 21:45:30 INFO SecurityManager: Changing modify acls to: yang
16/11/07 21:45:30 INFO SecurityManager: Changing view acls groups to:
16/11/07 21:45:30 INFO SecurityManager: Changing modify acls groups to:
16/11/07 21:45:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yang); groups with view permissions: Set(); users  with modify permissions: Set(yang); groups with modify permissions: Set()
16/11/07 21:45:31 INFO Utils: Successfully started service 'sparkDriver' on port 49977.
16/11/07 21:45:31 INFO SparkEnv: Registering MapOutputTracker
16/11/07 21:45:31 INFO SparkEnv: Registering BlockManagerMaster
16/11/07 21:45:31 INFO DiskBlockManager: Created local directory at /private/var/folders/8v/9_5lvzpd7vsfmvpq89vds7b80000gn/T/blockmgr-9dc4a7f5-0a80-44b2-9c06-438211b1d797
16/11/07 21:45:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/11/07 21:45:31 INFO SparkEnv: Registering OutputCommitCoordinator
16/11/07 21:45:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/11/07 21:45:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.4:4040
16/11/07 21:45:31 INFO SparkContext: Added JAR file:/Users/yang/Documents/BitTiger/CS202-1603/spark-streaming-kafka-0-8-assembly_2.11-2.0.1.jar at spark://192.168.1.4:49977/jars/spark-streaming-kafka-0-8-assembly_2.11-2.0.1.jar with timestamp 1478573131483
16/11/07 21:45:31 INFO SparkContext: Added file file:/Users/yang/Documents/BitTiger/CS202-1603/stream-processing.py at file:/Users/yang/Documents/BitTiger/CS202-1603/stream-processing.py with timestamp 1478573131715
16/11/07 21:45:31 INFO Utils: Copying /Users/yang/Documents/BitTiger/CS202-1603/stream-processing.py to /private/var/folders/8v/9_5lvzpd7vsfmvpq89vds7b80000gn/T/spark-86a0eaec-2c0f-49c3-be71-8d5c348bd5d3/userFiles-ae3baa82-75f5-4420-9e38-347eaa1bf3bc/stream-processing.py
16/11/07 21:45:31 INFO Executor: Starting executor ID driver on host localhost
16/11/07 21:45:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49978.
16/11/07 21:45:31 INFO NettyBlockTransferService: Server created on 192.168.1.4:49978
16/11/07 21:45:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.4, 49978)
16/11/07 21:45:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.4:49978 with 366.3 MB RAM, BlockManagerId(driver, 192.168.1.4, 49978)
16/11/07 21:45:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.4, 49978)
  ```

- The `data-producer.py` also need to be started and served as the back end by executing
  ```
  python data-producer.py
  ```
  which shows
  ```
  016-11-07 21:46:22,248  * Debugger is active!
  ```

