# Homework 1-2

### How to insure the High Avaibility and High Performance when deploying Kafka on AWS?
  - Geolocation
    - Kafka and Zookeeper should be depolyed into different geolocation such as U.S. East (N. Virgina) and U.S. Ease(Ohio)
  - Avaibility Zone
    - Distribute ZooKeeper nodes across multiple availability zones
    - Distribute Kafka brokers across multiple availability zones
  - EBS (Elastic Block Store)
    - In summary, using EBS volumes will decrease network traffic when a broker fails or is replaced. Also, the replacement broker will join the cluster more quickly. However, EBS volumes add cost to your AWS deployment.
    - In a simple Kafka benchmark, we saw better performance with st1 EBS than instance storage. st1 EBS is optimized for throughput.
    - Provision the block storage required to accommodate your streaming data throughput using Amazon Elastic Block Store (EBS)
    - Kafka brokers should be configured with dedicated disks, to limit disk thrashing and increase throughput. More specifically, log.dirs should only contain disks (or EBS volumes) that are dedicated to Kafka.
  - References
    - [Apache Kafka on AWS](https://aws.amazon.com/kafka/)
    - [Design and Deployment Considerations for Deploying Apache Kafka on AWS](http://www.confluent.io/blog/design-and-deployment-considerations-for-deploying-apache-kafka-on-aws/)
    - [Deploying Apache Kafka on AWS Elastic Block Store (EBS)](http://www.confluent.io/blog/deploying-apache-kafka-on-aws-elastic-block-store-ebs/)

### Create Python Development Environment
-----

- Use `virtualenv` to create the Python development environment in a specific folder by
  ```
  virtualenv env
  ```
  shows
  ```
  verwriting /Users/yang/Documents/BitTiger/CS202-1603/env/lib/python2.7/orig-prefix.txt with new content
  New python executable in /Users/yang/Documents/BitTiger/CS202-1603/env/bin/python2.7
  Not overwriting existing python script /Users/yang/Documents/BitTiger/CS202-1603/env/bin/python (you must use
  /Users/yang/Documents/BitTiger/CS202-1603/env/bin/python2.7)
  Installing setuptools, pip, wheel...done.
  ```
  
  then activating the environment by
  ```
  source /env/bin/activate
  ```
  shows
  ```
  (env)  ~/Documents/BitTiger/CS202-1603
  ```
  
  list all packages by
  ```
  pip list
  ```
  shows
  ```
  APScheduler (3.2.0)
click (6.6)
Flask (0.11.1)
funcsigs (1.0.2)
futures (3.0.5)
googlefinance (0.7)
itsdangerous (0.24)
Jinja2 (2.8)
kafka-python (1.3.1)
MarkupSafe (0.23)
pip (8.1.2)
pytz (2016.7)
setuptools (28.6.0)
six (1.10.0)
tzlocal (1.3)
Werkzeug (0.11.11)
wheel (0.30.0a0)
  ```

  freeze the development environment by
  ```
  pip freeze > requirements.txt
  ```
  and `cat requirements.txt` shows
  ```
  APScheduler==3.2.0
click==6.6
Flask==0.11.1
funcsigs==1.0.2
futures==3.0.5
googlefinance==0.7
itsdangerous==0.24
Jinja2==2.8
kafka-python==1.3.1
MarkupSafe==0.23
pytz==2016.7
six==1.10.0
tzlocal==1.3
Werkzeug==0.11.11
  ```

*NOTE*
  - This terminal will be used for executing `data-producer.py`
  - Create another identical one for executing `data-consumer.py`

### Read stock data from google finance
-----
  - Export the configuration file to `data-producer.py` by
  ```
  export ENV_CONFIG_FILE=/Users/yang/Documents/BitTiger/CS202-1603/config/dev.cfg
  ```
  shows
  ```
  ```
  
  then run
  ```
  python data-producer.py
  ```
  shows
  ```
  2016-10-26 11:49:55,430  * Debugger is active!
  ```
  
  In another identical terminal environment execute
  ```
  python data-consumer.py
  ```
  shows
  ```
  ```
  
  Open `Postman` and send a `POST` http request with url
  ```
  localhost:5000/GOOG
  ```
  
  The producer terminal shows
  ```
  2016-10-26 11:50:22,528 Start to fetch stock price for GOOG.
2016-10-26 11:50:23,026 Retrived stock price for GOOG.
2016-10-26 11:50:23,026 Finished writing stock price for GOOG to kafka.
  ```
  
  The consumer terminal shows
  ```
  ConsumerRecord(topic=u'stock-analyzer', partition=0, offset=610, timestamp=1477506142176, timestamp_type=0, key=None, value='[{"Index": "NASDAQ", "LastTradeWithCurrency": "798.14", "LastTradeDateTime": "2016-10-26T14:22:17Z", "LastTradePrice": "798.14", "LastTradeTime": "2:22PM EDT", "LastTradeDateTimeLong": "Oct 26, 2:22PM EDT", "StockSymbol": "GOOG", "ID": "304466804484872"}]', checksum=670335364, serialized_key_size=-1, serialized_value_size=255)
  ```

### Send stock data to Kafka (host and topic should be configurable)
-----
- Topics and hosts, as well as the debugging level can be configured via `dev.cfg` and `prd.cfg` in the `config` folder

### Able to add/delete more stocks
-----
-   More topics can be added as well as deleted through `Postman` using `POST` and `DELETE` method, respectively.
